{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"iacs.png\"> S-109A Introduction to Data Science: \n",
    "\n",
    "## Homework 5: Logistic Regression, High Dimensionality and PCA, LDA/QDA\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Summer 2018**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "Names of people you have worked with goes here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import math\n",
    "from scipy.special import gamma\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancer Classification from Gene Expressions\n",
    "\n",
    "In this problem, we will build a classification model to distinguish between two related classes of cancer, acute lymphoblastic leukemia (ALL) and acute myeloid leukemia (AML), using gene expression measurements. The data set is provided in the file `dataset_hw5_1.csv`. Each row in this file corresponds to a tumor tissue sample from a patient with one of the two forms of Leukemia. The first column contains the cancer type, with 0 indicating the ALL class and 1 indicating the AML class. Columns 2-7130 contain expression levels of 7129 genes recorded from each tissue sample. \n",
    "\n",
    "In the following questions, we will use linear and logistic regression to build a classification models for this data set. We will also use Principal Components Analysis (PCA) to visualize the data and to reduce its dimensions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Data Exploration\n",
    "\n",
    "1. First step is to  split  the observations into an approximate 50-50 train-test split.  Below is some code to do this for you (we want to make sure everyone has the same splits).\n",
    "\n",
    "2. Take a peek at your training set: you should notice the severe differences in the measurements from one gene to the next (some are negative, some hover around zero, and some are well into the thousands).  To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1.\n",
    "\n",
    "3. Notice that the results training set contains more predictors than observations. Do you foresee a problem in fitting a classification model to such a data set?\n",
    "\n",
    "4. Lets explore a few of the genes and see how well they discriminate between cancer classes. Create a single figure with four subplots arranged in a 2x2 grid. Consider the following four genes: `D29963_at`, `M23161_at`, `hum_alu_at`, and `AFFX-PheX-5_at`. For each gene overlay two histograms of the gene expression values on one of the subplots, one histogram for each cancer type. Does it appear that any of these genes discriminate between the two classes well? How are you able to tell?\n",
    "\n",
    "5. Since our data has dimensions that are not easily visualizable, we want to reduce the dimensionality of the data to make it easier to visualize. Using PCA, find the top two principal components for the gene expression data. Generate a scatter plot using these principal components, highlighting the two cancer types in different colors. How well do the top two principal components discriminate between the two classes? How much of the variance within the data do these two principal components explain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers:\n",
    "**1.1:** First step is to split the observations into an approximate 50-50 train-test split. Below is some code to do this for you (we want to make sure everyone has the same splits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9002)\n",
    "df = pd.read_csv('data/dataset_hw5_1.csv')\n",
    "msk = np.random.rand(len(df)) < 0.5\n",
    "data_train = df[msk]\n",
    "data_test = df[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2:** Take a peek at your training set: you should notice the severe differences in the measurements from one gene to the next (some are negative, some hover around zero, and some are well into the thousands).  To account for these differences in scale and variability, normalize each predictor to vary between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cancer_type</th>\n",
       "      <th>AFFX-BioB-5_at</th>\n",
       "      <th>AFFX-BioB-M_at</th>\n",
       "      <th>AFFX-BioB-3_at</th>\n",
       "      <th>AFFX-BioC-5_at</th>\n",
       "      <th>AFFX-BioC-3_at</th>\n",
       "      <th>AFFX-BioDn-5_at</th>\n",
       "      <th>AFFX-BioDn-3_at</th>\n",
       "      <th>AFFX-CreX-5_at</th>\n",
       "      <th>AFFX-CreX-3_at</th>\n",
       "      <th>...</th>\n",
       "      <th>U48730_at</th>\n",
       "      <th>U58516_at</th>\n",
       "      <th>U73738_at</th>\n",
       "      <th>X06956_at</th>\n",
       "      <th>X16699_at</th>\n",
       "      <th>X83863_at</th>\n",
       "      <th>Z17240_at</th>\n",
       "      <th>L49218_f_at</th>\n",
       "      <th>M71243_f_at</th>\n",
       "      <th>Z78285_f_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.466192</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.246154</td>\n",
       "      <td>0.433190</td>\n",
       "      <td>0.240418</td>\n",
       "      <td>0.880427</td>\n",
       "      <td>0.625850</td>\n",
       "      <td>0.928074</td>\n",
       "      <td>...</td>\n",
       "      <td>185</td>\n",
       "      <td>511</td>\n",
       "      <td>-125</td>\n",
       "      <td>389</td>\n",
       "      <td>-37</td>\n",
       "      <td>793</td>\n",
       "      <td>329</td>\n",
       "      <td>36</td>\n",
       "      <td>191</td>\n",
       "      <td>-37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.658363</td>\n",
       "      <td>0.794521</td>\n",
       "      <td>0.213953</td>\n",
       "      <td>0.421978</td>\n",
       "      <td>0.573276</td>\n",
       "      <td>0.717770</td>\n",
       "      <td>0.741637</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.505800</td>\n",
       "      <td>...</td>\n",
       "      <td>156</td>\n",
       "      <td>649</td>\n",
       "      <td>57</td>\n",
       "      <td>504</td>\n",
       "      <td>-26</td>\n",
       "      <td>250</td>\n",
       "      <td>314</td>\n",
       "      <td>14</td>\n",
       "      <td>56</td>\n",
       "      <td>-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.727758</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.586047</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.683190</td>\n",
       "      <td>0.649826</td>\n",
       "      <td>0.642705</td>\n",
       "      <td>0.736961</td>\n",
       "      <td>0.338747</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>224</td>\n",
       "      <td>60</td>\n",
       "      <td>194</td>\n",
       "      <td>-10</td>\n",
       "      <td>291</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>-2</td>\n",
       "      <td>-80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622309</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.200431</td>\n",
       "      <td>0.526132</td>\n",
       "      <td>0.708897</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0.570766</td>\n",
       "      <td>...</td>\n",
       "      <td>241</td>\n",
       "      <td>1214</td>\n",
       "      <td>127</td>\n",
       "      <td>255</td>\n",
       "      <td>50</td>\n",
       "      <td>1701</td>\n",
       "      <td>1108</td>\n",
       "      <td>61</td>\n",
       "      <td>525</td>\n",
       "      <td>-83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.702847</td>\n",
       "      <td>0.745597</td>\n",
       "      <td>0.113953</td>\n",
       "      <td>0.224176</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.620209</td>\n",
       "      <td>0.713167</td>\n",
       "      <td>0.705215</td>\n",
       "      <td>0.566125</td>\n",
       "      <td>...</td>\n",
       "      <td>186</td>\n",
       "      <td>573</td>\n",
       "      <td>-57</td>\n",
       "      <td>694</td>\n",
       "      <td>-19</td>\n",
       "      <td>636</td>\n",
       "      <td>205</td>\n",
       "      <td>17</td>\n",
       "      <td>127</td>\n",
       "      <td>-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Cancer_type  AFFX-BioB-5_at  AFFX-BioB-M_at  AFFX-BioB-3_at  \\\n",
       "0           0.0        0.466192        0.739726        0.255814   \n",
       "2           0.0        0.658363        0.794521        0.213953   \n",
       "5           0.0        0.727758        0.857143        0.586047   \n",
       "9           0.0        0.000000        0.622309        0.348837   \n",
       "10          0.0        0.702847        0.745597        0.113953   \n",
       "\n",
       "    AFFX-BioC-5_at  AFFX-BioC-3_at  AFFX-BioDn-5_at  AFFX-BioDn-3_at  \\\n",
       "0         0.246154        0.433190         0.240418         0.880427   \n",
       "2         0.421978        0.573276         0.717770         0.741637   \n",
       "5         0.107692        0.683190         0.649826         0.642705   \n",
       "9         0.714286        0.200431         0.526132         0.708897   \n",
       "10        0.224176        0.741379         0.620209         0.713167   \n",
       "\n",
       "    AFFX-CreX-5_at  AFFX-CreX-3_at     ...       U48730_at  U58516_at  \\\n",
       "0         0.625850        0.928074     ...             185        511   \n",
       "2         0.748299        0.505800     ...             156        649   \n",
       "5         0.736961        0.338747     ...              48        224   \n",
       "9         0.698413        0.570766     ...             241       1214   \n",
       "10        0.705215        0.566125     ...             186        573   \n",
       "\n",
       "    U73738_at  X06956_at  X16699_at  X83863_at  Z17240_at  L49218_f_at  \\\n",
       "0        -125        389        -37        793        329           36   \n",
       "2          57        504        -26        250        314           14   \n",
       "5          60        194        -10        291         41            8   \n",
       "9         127        255         50       1701       1108           61   \n",
       "10        -57        694        -19        636        205           17   \n",
       "\n",
       "    M71243_f_at  Z78285_f_at  \n",
       "0           191          -37  \n",
       "2            56          -25  \n",
       "5            -2          -80  \n",
       "9           525          -83  \n",
       "10          127          -13  \n",
       "\n",
       "[5 rows x 7130 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(df):\n",
    "    mins = np.min(df, axis=0)\n",
    "    maxs = np.max(df, axis=0)\n",
    "    return (df - maxs) / (maxs - mins)\n",
    "\n",
    "dat_train = normalize(data_train)\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3:** Notice that the results training set contains more predictors than observations. Do you foresee a problem in fitting a classification model to such a data set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Since the training set contains more predictors than observations, fitting a classification model here is actually impossible.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4:** Lets explore a few of the genes and see how well they discriminate between cancer classes. Create a single figure with four subplots arranged in a 2x2 grid. Consider the following four genes: `D29963_at`, `M23161_at`, `hum_alu_at`, and `AFFX-PheX-5_at`. For each gene overlay two histograms of the gene expression values on one of the subplots, one histogram for each cancer type. Does it appear that any of these genes discriminate between the two classes well? How are you able to tell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train_ALL = data_train.query('Cancer_type==0')\n",
    "data_train_AML = data_train.query('Cancer_type==1')\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(12,12))\n",
    "\n",
    "fig.suptitle('Gene Expression in ALL and AML', fontsize=20)\n",
    "\n",
    "for r in range(2):\n",
    "    for c in range(2):\n",
    "        \n",
    "        axes[r,c].grid(False)\n",
    "        axes[r,c].set_facecolor('white')\n",
    "        \n",
    "        if r == 0:\n",
    "            \n",
    "            # Top Left\n",
    "            if c == 0:\n",
    "                axes[r,c].hist(data_train_ALL['D29963_at'], color='crimson', alpha=0.4, label='ALL')\n",
    "                axes[r,c].hist(data_train_AML['D29963_at'], color='blue', alpha=0.4, label='AML')\n",
    "\n",
    "                axes[r,c].set_ylabel('Frequency', fontsize=16)\n",
    "                axes[r,c].set_title('Gene D29963_at', fontsize=16)\n",
    "                \n",
    "            # Top Right\n",
    "            else:\n",
    "                axes[r,c].hist(data_train_ALL['M23161_at'], color='crimson', alpha=0.4)\n",
    "                axes[r,c].hist(data_train_AML['M23161_at'], color='blue', alpha=0.4)\n",
    "                \n",
    "                axes[r,c].set_title('Gene M23161_at', fontsize=16)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            axes[r,c].set_xlabel('Gene Expression', fontsize=16)\n",
    "            \n",
    "            # Bottom Left\n",
    "            if c == 0:\n",
    "                axes[r,c].hist(data_train_ALL['hum_alu_at'], color='crimson', alpha=0.4)\n",
    "                axes[r,c].hist(data_train_AML['hum_alu_at'], color='blue', alpha=0.4)\n",
    "                \n",
    "                axes[r,c].set_ylabel('Frequency', fontsize=16)\n",
    "                axes[r,c].set_title('Gene hum_alu_at', fontsize=16)\n",
    "                \n",
    "            # Bottom Right\n",
    "            else:\n",
    "                axes[r,c].hist(data_train_ALL['AFFX-PheX-5_at'], color='crimson', alpha=0.4)\n",
    "                axes[r,c].hist(data_train_AML['AFFX-PheX-5_at'], color='blue', alpha=0.4)\n",
    "                \n",
    "                axes[r,c].set_title('Gene AFFX-PheX-5_at', fontsize=16)\n",
    "                \n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Your answer here\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5:** Since our data has dimensions that are not easily visualizable, we want to reduce the dimensionality of the data to make it easier to visualize. Using PCA, find the top two principal components for the gene expression data. Generate a scatter plot using these principal components, highlighting the two cancer types in different colors. How well do the top two principal components discriminate between the two classes? How much of the variance within the data do these two principal components explain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Your answer here\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Linear Regression vs. Logistic Regression\n",
    "\n",
    "In class we discussed how to use both linear regression and logistic regression for classification. For this question, you will work with a single gene predictor, `D29963_at`, to explore these two methods.\n",
    "\n",
    "1. Fit a simple linear regression model to the training set using the single gene predictor `D29963_at`. We could interpret the scores predicted by the regression model interpreted for a patient as an estimate of the probability that the patient has `Cancer_type`=1. Is there a problem with this interpretation?\n",
    "\n",
    "2. The fitted linear regression model can be converted to a classification model (i.e. a model that predicts one of two binary labels 0 or 1) by classifying patients with predicted score greater than 0.5 into `Cancer_type`=1, and the others into the `Cancer_type`=0. Evaluate the classification accuracy (1 - misclassification rate) of the obtained classification model on both the training and test sets.\n",
    "\n",
    "3. Next, fit a simple logistic regression model to the training set. How do the training and test classification accuracies of this model compare with the linear regression model? Remember, you need to set the regularization parameter for sklearn's logistic regression function to be a very large value in order to not regularize (use 'C=100000').\n",
    "\n",
    "4. Plot the quantitative output from the linear regression model and the probabilistic output from the logistic regression model (on the training set points) as a function of the gene predictor. Also, display the true binary response for the training set points in the same plot. Based on these plots, does one of the models appear better suited for binary classification than the other? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers: \n",
    "\n",
    "**2.1:** Fit a simple linear regression model to the training set using the single gene predictor `D29963_at`. We could interpret the scores predicted by the regression model interpreted for a patient as an estimate of the probability that the patient has `Cancer_type`=1. Is there a problem with this interpretation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Your answer here\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2:** The fitted linear regression model can be converted to a classification model (i.e. a model that predicts one of two binary labels 0 or 1) by classifying patients with predicted score greater than 0.5 into `Cancer_type`=1, and the others into the `Cancer_type`=0. Evaluate the classification accuracy (1 - misclassification rate) of the obtained classification model on both the training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3:** Next, fit a simple logistic regression model to the training set. How do the training and test classification accuracies of this model compare with the linear regression model? Remember, you need to set the regularization parameter for sklearn's logistic regression function to be a very large value in order to not regularize (use 'C=100000')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Your answer here\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4:** Plot the quantitative output from the linear regression model and the probabilistic output from the logistic regression model (on the training set points) as a function of the gene predictor. Also, display the true binary response for the training set points in the same plot. Based on these plots, does one of the models appear better suited for binary classification than the other? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Your answer here\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Multiple Logistic Regression\n",
    "\n",
    "1. Next, fit a multiple logistic regression model with all the gene predictors from the data set.  How does the classification accuracy of this model compare with the models fitted in question 2 with a single gene (on both the training and test sets)?  \n",
    "\n",
    "2. Use the `visualize_prob` function provided below to visualize the probabilties predicted by the fitted multiple logistic regression model on both the training and test data sets. The function creates a visualization that places the data points on a vertical line based on the predicted probabilities, with the different cancer classes shown in different colors, and with the 0.5 threshold highlighted using a dotted horizontal line. Is there a difference in the spread of probabilities in the training and test plots? Are there data points for which the predicted probability is close to 0.5? If so, what can you say about these points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  visualize_prob\n",
    "# A function to visualize the probabilities predicted by a Logistic Regression model\n",
    "# Input: \n",
    "#      model (Logistic regression model)\n",
    "#      x (n x d array of predictors in training data)\n",
    "#      y (n x 1 array of response variable vals in training data: 0 or 1)\n",
    "#      ax (an axis object to generate the plot)\n",
    "\n",
    "def visualize_prob(model, x, y, ax):\n",
    "    # Use the model to predict probabilities for\n",
    "    y_pred = model.predict_proba(x)\n",
    "    \n",
    "    # Separate the predictions on the label 1 and label 0 points\n",
    "    ypos = y_pred[y==1]\n",
    "    yneg = y_pred[y==0]\n",
    "    \n",
    "    # Count the number of label 1 and label 0 points\n",
    "    npos = ypos.shape[0]\n",
    "    nneg = yneg.shape[0]\n",
    "    \n",
    "    # Plot the probabilities on a vertical line at x = 0, \n",
    "    # with the positive points in blue and negative points in red\n",
    "    pos_handle = ax.plot(np.zeros((npos,1)), ypos[:,1], 'bo', label = 'Cancer Type 1')\n",
    "    neg_handle = ax.plot(np.zeros((nneg,1)), yneg[:,1], 'ro', label = 'Cancer Type 0')\n",
    "\n",
    "    # Line to mark prob 0.5\n",
    "    ax.axhline(y = 0.5, color = 'k', linestyle = '--')\n",
    "    \n",
    "    # Add y-label and legend, do not display x-axis, set y-axis limit\n",
    "    ax.set_ylabel('Probability of AML class')\n",
    "    ax.legend(loc = 'best')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers \n",
    "**3.1:** Next, fit a multiple logistic regression model with all the gene predictors from the data set.  How does the classification accuracy of this model compare with the models fitted in question 2 with a single gene (on both the training and test sets)?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Your answer here\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2:** Use the `visualize_prob` function provided below to visualize the probabilties predicted by the fitted multiple logistic regression model on both the training and test data sets. The function creates a visualization that places the data points on a vertical line based on the predicted probabilities, with the different cancer classes shown in different colors, and with the 0.5 threshold highlighted using a dotted horizontal line. Is there a difference in the spread of probabilities in the training and test plots? Are there data points for which the predicted probability is close to 0.5? If so, what can you say about these points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Your answer here\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Analyzing Significance of Coefficients\n",
    "\n",
    "How many of the coefficients estimated by the multiple logistic regression in the previous problem are significantly different from zero at a *significance level of 95%*? \n",
    "\n",
    "Hint: To answer this question, use *bootstrapping* with 1000 boostrap samples/iterations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: High Dimensionality\n",
    "\n",
    "One of the issues you may run into when dealing with high dimensional data is that your 2D and 3D intuition may fail breakdown. For example, distance metrics in high dimensions can have properties that may feel counterintuitive.\n",
    "\n",
    "Consider the following: You have a hypersphere with a radius of $1$, inside of a hypercube centered at $0$, with edges of length $2$.\n",
    "\n",
    "1. As a function of $d$, the number of dimensions, how much of the hypercube's volume is contained within the hypersphere?\n",
    "2. What happens as $d$ gets very large?\n",
    "3. Using the functions provided below, create a plot of how the volume ratio changes as a function of $d$.\n",
    "4. What does this tell you about where the majority of the volume of the hypercube resides in higher dimensions? \n",
    "\n",
    "*HINTS:* \n",
    "- The volume of a hypercube with edges of length $2$ is $V_c(d) = 2^d$.\n",
    "- The volume of a hyperphere with a radius of $1$ is $V_s(d) = \\frac{\\pi^{\\frac{d}{2}}} {\\Gamma(\\frac{d}{2}+1)}$, where $\\Gamma$ is Euler's Gamma Function.\n",
    "- $\\Gamma$ is increasing for all $d \\geq 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def V_c(d):\n",
    "    \"\"\"\n",
    "    Calculate the volumn of a hypercube of dimension d.\n",
    "    \"\"\"\n",
    "    return 2**d\n",
    "\n",
    "def V_s(d):\n",
    "    \"\"\"\n",
    "    Calculate the volume of a hypersphere of dimension d.\n",
    "    \"\"\"\n",
    "    return math.pi**(d/2)/gamma((d/2)+1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Your answer here\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: PCA and Dimensionality Reduction\n",
    "\n",
    "As we saw above, high dimensional problems can have counterintuitive behavior, thus we often want to try to reduce the dimensionality of our problems. A reasonable approach to reduce the dimensionality of the data is to use PCA and fit a logistic regression model on the smallest set of principal components that explain at least 90% of the variance in the predictors.\n",
    "\n",
    "1. Using the gene data from Problem 1, how many principal components do we need to capture at least 90% of the variance? How much of the variance do they actually capture? Fit a Logistic Regression model using these principal components. How do the classification accuracy values on both the training and tests sets compare with the models fit in question 3.1?  \n",
    "\n",
    "2. Use the code provided in question 3 to visualize the probabilities predicted by the fitted model on both the training and test sets. How does the spread of probabilities in these plots compare to those for the model in question 3.2? If the lower dimensional representation yields comparable predictive power, what advantage does the lower dimensional representation provide?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers:\n",
    "\n",
    "**6.1:** Using the gene data from Problem 1, how many principal components do we need to capture at least 90% of the variance? How much of the variance do they actually capture? Fit a Logistic Regression model using these principal components. How do the classification accuracy values on both the training and tests sets compare with the models fit in question 3.1?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Your answer here\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2:** Use the code provided in question 3 to visualize the probabilities predicted by the fitted model on both the training and test sets. How does the spread of probabilities in these plots compare to those for the model in question 3.2? If the lower dimensional representation yields comparable predictive power, what advantage does the lower dimensional representation provide?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Your answer here\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Multiclass Thyroid Classification\n",
    "\n",
    "In this problem, you will build a model for diagnosing disorders in a patient's thyroid gland. Given the results of medical tests on a patient, the task is to classify the patient either as:\n",
    "- *normal* (class 1)\n",
    "- having *hyperthyroidism* (class 2)\n",
    "- or having *hypothyroidism* (class 3). \n",
    "\n",
    "The data set is provided in the file `dataset_hw5_2.csv`. Columns 1-2 contain biomarkers for a patient (predictors):\n",
    "- Biomarker 1: (Logarithm of) level of basal thyroid-stimulating hormone (TSH) as measured by radioimmuno assay\n",
    "- Biomarker 2: (Logarithm of) maximal absolute difference of TSH value after injection of 200 micro grams of thyrotropin-releasing hormone as compared to the basal value.\n",
    "\n",
    "The last column contains the diagnosis for the patient from a medical expert. This data set was obtained from the UCI Machine Learning Repository.\n",
    "\n",
    "Notice that unlike previous exercises, the task at hand is a 3-class classification problem. We will explore the use of different methods for multiclass classification.\n",
    "\n",
    "First task: split the data using the code provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Fit Classification Models\n",
    "\n",
    "1. Generate a 2D scatter plot of the training set, denoting each class with a different color. Does it appear that the data points can be separated well by a linear classifier?\n",
    "\n",
    "2. Briefly explain the difference between multinomial logistic regression and one-vs-rest (OvR) logistic regression methods for fitting a multiclass classifier (in 2-3 sentences).\n",
    "\n",
    "3. Fit linear classification models on the thyroid data set using both the methods.  You should use $L_2$ regularization in both cases, tuning the regularization parameter using cross-validation.  Is there a difference in the overall classification accuracy of the two methods on the test set?\n",
    "\n",
    "4. Also, compare the training and test accuracies of these models with the following classification methods:\n",
    "    - Multiclass Logistic Regression with quadratic terms \n",
    "    - Linear Discriminant Analysis\n",
    "    - Quadratic Discriminant Analysis\n",
    "    - k-Nearest Neighbors\n",
    " <br>\n",
    "*Note:* you may use either the OvR or multinomial variant for the multiclass logistic regression (with $L_2$ regularization). Do not forget to use cross-validation to choose the regularization parameter, and also the number of neighbors in k-NN. \n",
    "\n",
    "5. Does the inclusion of the polynomial terms in logistic regression yield better test accuracy compared to the model with only linear terms? \n",
    "\n",
    "\n",
    "*Hint:* You may use the `KNeighborsClassifier` class to fit a k-NN classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers:\n",
    "\n",
    "**7.0:** First task: split the data using the code provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(9001)\n",
    "df = pd.read_csv('data/dataset_hw5_2.csv')\n",
    "msk = np.random.rand(len(df)) < 0.5\n",
    "data_train = df[msk]\n",
    "data_test = df[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.1:** Generate a 2D scatter plot of the training set, denoting each class with a different color. Does it appear that the data points can be separated well by a linear classifier?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Your answer here\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.2:** Briefly explain the difference between multinomial logistic regression and one-vs-rest (OvR) logistic regression methods for fitting a multiclass classifier (in 2-3 sentences).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Your answer here\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.3:** Fit linear classification models on the thyroid data set using both the methods.  You should use $L_2$ regularization in both cases, tuning the regularization parameter using cross-validation.  Is there a difference in the overall classification accuracy of the two methods on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Your answer here\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4:** Also, compare the training and test accuracies of these models with the following classification methods:\n",
    "    - Multiclass Logistic Regression with quadratic terms \n",
    "    - Linear Discriminant Analysis\n",
    "    - Quadratic Discriminant Analysis\n",
    "    - k-Nearest Neighbors\n",
    "\n",
    "*Note:* you may use either the OvR or multinomial variant for the multiclass logistic regression (with $L_2$ regularization). Do not forget to use cross-validation to choose the regularization parameter, and also the number of neighbors in k-NN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.5:** Does the inclusion of the polynomial terms in logistic regression yield better test accuracy compared to the model with only linear terms? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Your answer here\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: Visualize Decision Boundaries\n",
    "\n",
    "The following code will allow you to visualize the decision boundaries of a given classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  plot_decision_boundary\n",
    "# A function that visualizes the data and the decision boundaries\n",
    "# Input: \n",
    "#      x (predictors)\n",
    "#      y (labels)\n",
    "#      model (the classifier you want to visualize)\n",
    "#      title (title for plot)\n",
    "#      ax (a set of axes to plot on)\n",
    "#      poly_degree (highest degree of polynomial terms included in the model; None by default)\n",
    "\n",
    "def plot_decision_boundary(x, y, model, title, ax, poly_degree=None):\n",
    "    # Create mesh\n",
    "    # Interval of points for biomarker 1\n",
    "    min0 = x[:,0].min()\n",
    "    max0 = x[:,0].max()\n",
    "    interval0 = np.arange(min0, max0, (max0-min0)/100)\n",
    "    n0 = np.size(interval0)\n",
    "    \n",
    "    # Interval of points for biomarker 2\n",
    "    min1 = x[:,1].min()\n",
    "    max1 = x[:,1].max()\n",
    "    interval1 = np.arange(min1, max1, (max1-min1)/100)\n",
    "    n1 = np.size(interval1)\n",
    "\n",
    "    # Create mesh grid of points\n",
    "    x1, x2 = np.meshgrid(interval0, interval1)\n",
    "    x1 = x1.reshape(-1,1)\n",
    "    x2 = x2.reshape(-1,1)\n",
    "    xx = np.concatenate((x1, x2), axis=1)\n",
    "\n",
    "    # Predict on mesh of points\n",
    "    # Check if polynomial terms need to be included\n",
    "    if(poly_degree!=None):\n",
    "        # Use PolynomialFeatures to generate polynomial terms\n",
    "        poly = PolynomialFeatures(poly_degree,include_bias = False)\n",
    "        xx_ = poly.fit_transform(xx)\n",
    "        yy = model.predict(xx_) \n",
    "        \n",
    "    else:   \n",
    "        yy = model.predict(xx)\n",
    "        \n",
    "    yy = yy.reshape((n0, n1))\n",
    "\n",
    "    # Plot decision surface\n",
    "    x1 = x1.reshape(n0, n1)\n",
    "    x2 = x2.reshape(n0, n1)\n",
    "    ax.contourf(x1, x2, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    \n",
    "    # Plot scatter plot of data\n",
    "    yy = y.reshape(-1,)\n",
    "    ax.scatter(x[yy==1,0], x[yy==1,1], c='blue', label='Normal', cmap=plt.cm.coolwarm)\n",
    "    ax.scatter(x[yy==2,0], x[yy==2,1], c='cyan', label='Hyper', cmap=plt.cm.coolwarm)\n",
    "    ax.scatter(x[yy==3,0], x[yy==3,1], c='red', label='Hypo', cmap=plt.cm.coolwarm)\n",
    "    \n",
    "    # Label axis, title\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Biomarker 1')\n",
    "    ax.set_ylabel('Biomarker 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The provided code uses `sklearn`'s `PolynomialFeatures` to generate higher-order polynomial terms, with degree `poly_degree`. \n",
    "Also, if you have loaded the data sets into `pandas` data frames, you may use the `as_matrix` function to obtain a `numpy` array from the data frame objects.\n",
    "\n",
    "1. Use the above code to visualize the decision boundaries for each of the model fitted in the previous question.\n",
    "2. Comment on the difference in the decision boundaries (if any) for the OvR and multinomial logistic regression models. Is there a difference between the decision boundaries for the linear logistic regression models and LDA. What about the decision boundaries for the quadratic logistic regression and QDA? Give an explanation for your answer.\n",
    "3. QDA is a generalization of the LDA model. What's the primary difference that makes QDA more general? How does that manifest in the plots you generated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answers:\n",
    "\n",
    "**8.1:** Use the above code to visualize the decision boundaries for each of the model fitted in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2:** Comment on the difference in the decision boundaries (if any) for the OvR and multinomial logistic regression models. Is there a difference between the decision boundaries for the linear logistic regression models and LDA. What about the decision boundaries for the quadratic logistic regression and QDA? Give an explanation for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Your answer here\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.3:** QDA is a generalization of the LDA model. What's the primary difference that makes QDA more general? How does that manifest in the plots you generated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Your answer here\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
